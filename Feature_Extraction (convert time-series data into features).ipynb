{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import * \n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SparkSession.builder.master(\"local[*]\").appName(\"appName\").\\\n",
    "                      config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/dwoodbridge/Research/Medhere/Data\"\n",
    "\n",
    "percentage_to_drop_in_file = 0.25\n",
    "\n",
    "#temporary files\n",
    "temp_trimmed_file = \"temp.csv\"\n",
    "temp_accelerometer_file = \"temp_accelerometer.csv\"\n",
    "temp_gyroscope_file = \"temp_gyroscope.csv\" \n",
    "\n",
    "#final output file\n",
    "final_output_file = \"discretizaed_data.csv\"\n",
    "\n",
    "new_bin_no = 5 # how many bins it will use. \n",
    "time_unit = 1000000 # the basic unit of time increment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_file(file_name, percentage_to_drop_in_file):\n",
    "    file = open(os.path.join(root, name), \"rw\")\n",
    "    input_string = file.read()\n",
    "    line = input_string.split(\"\\n\")\n",
    "    total_line = len(line)\n",
    "    print total_line\n",
    "    extracted_line = line[(int) (total_line * percentage_to_drop_in_file):\\\n",
    "                          (int) (total_line *(1 - percentage_to_drop_in_file))]\n",
    "    with open(temp_trimmed_file, 'w') as file_handler:\n",
    "        for item in extracted_line:\n",
    "            file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_trimmed_file_as_df():\n",
    "    schema = StructType([\n",
    "            StructField(\"time_stamp\", DoubleType(), True),\n",
    "            StructField(\"sensor_type\", IntegerType(), True),\n",
    "            StructField(\"sensor_name\", StringType(), True),\n",
    "            StructField(\"x_val\", DoubleType(), True),\n",
    "            StructField(\"y_val\", DoubleType(), True),\n",
    "            StructField(\"z_val\", DoubleType(), True)])\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "                        .appName(\"Data_Prep\") \\\n",
    "                        .master(\"local[*]\") \\\n",
    "                        .config(conf=SparkConf()) \\\n",
    "                        .getOrCreate()\n",
    "        \n",
    "    df_motion = spark.read \\\n",
    "                    .format(\"csv\") \\\n",
    "                    .option(\"delimiter\", \",\") \\\n",
    "                    .load(temp_trimmed_file, schema=schema)\n",
    "    return df_motion       \n",
    "    #df_motion.cache() #cahcing in memory ==> enhance the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_min_max(df_motion):\n",
    "    time_min = df_motion.sort(\"time_stamp\", ascending=1).select('time_stamp').first()\n",
    "    time_min = time_min[0]\n",
    "    time_max = df_motion.sort(\"time_stamp\", ascending=0).select('time_stamp').first()\n",
    "    time_max = time_max[0]\n",
    "    return {'time_min':time_min, 'time_max':time_max }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaging_sensor_data(df_motion,sensor_type):\n",
    "    return df_motion.filter('sensor_type='+str(sensor_type)).\\\n",
    "                     groupBy('time_stamp').\\\n",
    "                     avg('x_val','y_val','z_val').\\\n",
    "                     sort(\"time_stamp\", ascending=1).\\\n",
    "                     withColumnRenamed('avg(x_val)', 'x_val').\\\n",
    "                     withColumnRenamed('avg(y_val)', 'y_val').\\\n",
    "                     withColumnRenamed('avg(z_val)', 'z_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def missing_data_imputation(sensor_values, sensor_type):\n",
    "    #DO NOT INTERPORATE : PROBLEM IS THAT SENSOR IS ONLY WRITTEN WHEN DATA POINT IS CHANGED.\n",
    "    if(sensor_type == 1):\n",
    "        file_name = temp_accelerometer_file\n",
    "    elif(sensor_type ==4):\n",
    "        file_name = temp_gyroscope_file\n",
    "\n",
    "    first = sensor_values.first()\n",
    "\n",
    "    print time_min\n",
    "    print time_max\n",
    "    with open(file_name, 'w') as file_handler:\n",
    "        if(first[0] > time_min):\n",
    "            time = time_min\n",
    "        else :\n",
    "            time = first[0]\n",
    "        while(first[0] > time):\n",
    "                item =  str(first[1]) + \",\" + str(first[2]) + \",\" + str(first[3]) \n",
    "                #print \"Boo\"+ str(time) + \",\" + item\n",
    "                file_handler.write(\"{}\\n\".format(str(time) + \",\" + item))\n",
    "                time = time + time_unit       \n",
    "\n",
    "        for row in sensor_values.collect():\n",
    "            while(time < row.time_stamp):\n",
    "                    #print  \"Zoo\" + str(time) + \",\" + item\n",
    "                    file_handler.write(\"{}\\n\".format(str(time) + \",\" + item))\n",
    "                    time = time + time_unit\n",
    "            item =  str(row[1]) + \",\" + str(row[2]) + \",\" + str(row[3])\n",
    "            #print \"Coo\" + str(time) + \",\" +item  \n",
    "            file_handler.write(\"{}\\n\".format(str(time) + \",\" + item))\n",
    "            time = time + time_unit\n",
    "\n",
    "        while(time <= time_max):\n",
    "            #print \"ZEE\" + str(time) + \",\" +item  \n",
    "            file_handler.write(\"{}\\n\".format(str(time) + \",\" + item))\n",
    "            time = time + time_unit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_stat(sensor_type):\n",
    "    if(sensor_type == 1):\n",
    "        file_name = temp_accelerometer_file\n",
    "    elif(sensor_type ==4):\n",
    "        file_name = temp_gyroscope_file\n",
    "\n",
    "    schema = StructType([\n",
    "                StructField(\"time_stamp\", DoubleType(), True),\n",
    "                StructField(\"x_val\", DoubleType(), True),\n",
    "                StructField(\"y_val\", DoubleType(), True),\n",
    "                StructField(\"z_val\", DoubleType(), True)])\n",
    "\n",
    "    df_after_missing_data_imputation = spark.read \\\n",
    "                                            .format(\"csv\") \\\n",
    "                                            .option(\"delimiter\", \",\") \\\n",
    "                                            .load(file_name, schema=schema)\n",
    "                \n",
    "    #structure : timestamp, x_val, y_val, z_val\n",
    "    global_avg = df_after_missing_data_imputation.groupBy().avg().rdd.flatMap(lambda x:  x).collect()\n",
    "    global_avg_x_val = global_avg[1]\n",
    "    global_avg_y_val = global_avg[2]\n",
    "    global_avg_z_val = global_avg[3]\n",
    "\n",
    "    global_min = df_after_missing_data_imputation.groupBy().min().rdd.flatMap(lambda x:  x).collect()\n",
    "    global_min_x_val = global_min[1]\n",
    "    global_min_y_val = global_min[2]\n",
    "    global_min_z_val = global_min[3]\n",
    "\n",
    "    global_max = df_after_missing_data_imputation.groupBy().max().rdd.flatMap(lambda x:  x).collect()\n",
    "    global_max_x_val = global_max[1]\n",
    "    global_max_y_val = global_max[2]\n",
    "    global_max_z_val = global_max[3]\n",
    "\n",
    "    global_std_x_val = df_after_missing_data_imputation.groupBy().agg(stddev(\"x_val\")).rdd.flatMap(lambda x:  x).collect()[0]\n",
    "    global_std_y_val = df_after_missing_data_imputation.groupBy().agg(stddev(\"y_val\")).rdd.flatMap(lambda x:  x).collect()[0]\n",
    "    global_std_z_val = df_after_missing_data_imputation.groupBy().agg(stddev(\"z_val\")).rdd.flatMap(lambda x:  x).collect()[0]\n",
    "\n",
    "    global_med_x_val = df_after_missing_data_imputation.approxQuantile(\"x_val\", [0.5], 0)[0]\n",
    "    global_med_y_val = df_after_missing_data_imputation.approxQuantile(\"y_val\", [0.5], 0)[0]\n",
    "    global_med_z_val = df_after_missing_data_imputation.approxQuantile(\"z_val\", [0.5], 0)[0]\n",
    "\n",
    "    global_stat =  str(global_min_x_val) + \",\" + str(global_min_y_val) + \",\" + str(global_min_z_val) + \",\" +\\\n",
    "                   str(global_med_x_val) + \",\" + str(global_med_y_val) + \",\" + str(global_med_z_val) + \",\" +\\\n",
    "                   str(global_max_x_val) + \",\" + str(global_max_y_val) + \",\" + str(global_max_z_val) + \",\" +\\\n",
    "                   str(global_avg_x_val) + \",\" + str(global_avg_y_val) + \",\" + str(global_avg_z_val) + \",\" +\\\n",
    "                   str(global_std_x_val) + \",\" + str(global_std_y_val) + \",\" + str(global_std_z_val) \n",
    "    return global_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_stat(old_bin_no,sensor_type):\n",
    "    if(sensor_type == 1):\n",
    "        file_name = temp_accelerometer_file\n",
    "    elif(sensor_type ==4):\n",
    "        file_name = temp_gyroscope_file\n",
    "\n",
    "    schema = StructType([\n",
    "                StructField(\"time_stamp\", DoubleType(), True),\n",
    "                StructField(\"x_val\", DoubleType(), True),\n",
    "                StructField(\"y_val\", DoubleType(), True),\n",
    "                StructField(\"z_val\", DoubleType(), True)])\n",
    "\n",
    "    df_after_missing_data_imputation = spark.read \\\n",
    "                                            .format(\"csv\") \\\n",
    "                                            .option(\"delimiter\", \",\") \\\n",
    "                                            .load(file_name, schema=schema)\n",
    "                \n",
    "    data_in_each_bin = int(old_bin_no/new_bin_no)\n",
    "    count = 0\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    line = ''\n",
    "    for val in df_after_missing_data_imputation.rdd.map(lambda x : x).collect():\n",
    "      count = count + 1\n",
    "      x_list.append(val.x_val)\n",
    "      y_list.append(val.y_val)\n",
    "      z_list.append(val.z_val)\n",
    "      if( count % data_in_each_bin == 0 ):\n",
    "          bin_stat =  str(np.min(x_list)) + \",\" + str(np.min(y_list)) + \",\" + str(np.min(z_list)) + \",\" + \\\n",
    "                      str(np.median(x_list)) + \",\" + str(np.median(y_list)) + \",\" + str(np.median(z_list)) + \",\" +\\\n",
    "                      str(np.max(x_list)) + \",\" +  str(np.max(y_list)) + \",\" + str(np.max(z_list)) + \",\" + \\\n",
    "                      str(np.mean(x_list)) + \",\" + str(np.mean(y_list)) + \",\" + str(np.mean(z_list)) + \",\" +\\\n",
    "                      str(np.std(x_list)) + \",\" + str(np.std(y_list)) + \",\" + str(np.std(z_list)) \n",
    "          if(count == data_in_each_bin):\n",
    "            line = bin_stat\n",
    "          else:\n",
    "            line = line + \",\" + bin_stat\n",
    "          print bin_stat\n",
    "          x_list=[]\n",
    "          y_list=[]\n",
    "          z_list=[]\n",
    "            \n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITERATE EACH FILE TO TRIM, IMPUTE MISSING DATA AND DISCRETIZE DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "with open(final_output_file, 'w') as file_handler:\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for name in files:\n",
    "            if re.search('.*dominant.*(.csv)$', name):            \n",
    "                file_label =  re.search('[a-z_]+',name).group(0)[:-1]\n",
    "                line = file_label\n",
    "                print file_label + \" : \" + name\n",
    "                #print(os.path.join(root, name))\n",
    "\n",
    "                #trim_file\n",
    "                trim_file(os.path.join(root, name), percentage_to_drop_in_file)\n",
    "\n",
    "                df_motion = read_trimmed_file_as_df()\n",
    "                df_motion.cache() #cahcing in memory ==> enhance the performance.\n",
    "\n",
    "                #claculate time_min, time_max, and old_bin_no\n",
    "                df_time_calcaultion = time_min_max(df_motion)\n",
    "                time_min =  df_time_calcaultion['time_min']\n",
    "                time_max =  df_time_calcaultion['time_max']\n",
    "                time_diff = time_max-time_min\n",
    "                print time_diff\n",
    "                #old_bin_no = time_diff/time_unit\n",
    "\n",
    "                ##missing_data_imputation\n",
    "                #sensor_types = [1,4] # accelerometer : 1, gyroscope : 4 \n",
    "                #for sensor_type in sensor_types:\n",
    "                #    sensor_values = averaging_sensor_data(df_motion, sensor_type)\n",
    "                #    missing_data_imputation(sensor_values, sensor_type)\n",
    "\n",
    "                #for sensor_type in sensor_types:\n",
    "                #    line = line + \",\" + global_stat(sensor_type)\n",
    "\n",
    "                #for sensor_type in sensor_types:\n",
    "                #    line = line + \",\" + moving_stat(old_bin_no,sensor_type)\n",
    "\n",
    "                #file_handler.write(\"{}\\n\".format(line))\n",
    "                \n",
    "                #df_motion.unpersist()   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
